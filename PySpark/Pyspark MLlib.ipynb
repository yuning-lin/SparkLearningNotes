{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327c5411",
   "metadata": {},
   "source": [
    "## Pyspark MLlib\n",
    "1. Read csv data\n",
    "2. Handling categorical features by `StringIndexer`, `OneHotEncoder` with `Pipeline`\n",
    "3. Use `VectorAssembler` create new column to assembling the indepent variables for modeling\n",
    "4. Split into training and testing --> fit model\n",
    "5. Get estimated parameters and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4228b676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AA1900286-NB.corpnet.asus:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_practice_MLlib</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18f5c128e10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark_practice_MLlib').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18dc2c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+------+----------+--------+\n",
      "|  name|sex|age|salary|experience|location|\n",
      "+------+---+---+------+----------+--------+\n",
      "|   amy|  F| 22| 28000|         1|  taipei|\n",
      "| jacky|  M| 20| 25000|         1|taichung|\n",
      "| meggy|  F| 30| 35000|         8|  tainan|\n",
      "|  adam|  M| 33| 34000|        10|  taipei|\n",
      "|sophie|  F| 40| 60000|        20|  taipei|\n",
      "|claire|  F| 25| 30000|         3|  tainan|\n",
      "|sherry|  F| 27| 51000|         6|  tainan|\n",
      "| phlip|  M| 31| 37000|         7|taichung|\n",
      "| fanny|  F| 29| 49000|         5|taichung|\n",
      "+------+---+---+------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['name', 'sex', 'age', 'salary', 'experience', 'location']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 1. Read data\n",
    "\n",
    "pyspark_df = spark.read.csv('./Data/customer_for_ml.csv', header='true', inferSchema=True)\n",
    "pyspark_df.show()\n",
    "pyspark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb8ebd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+------+----------+--------+-----------+----------------+-----------------------+\n",
      "|  name|sex|age|salary|experience|location|indexed_sex|indexed_location|onehot_indexed_location|\n",
      "+------+---+---+------+----------+--------+-----------+----------------+-----------------------+\n",
      "|   amy|  F| 22| 28000|         1|  taipei|        0.0|             2.0|              (2,[],[])|\n",
      "| jacky|  M| 20| 25000|         1|taichung|        1.0|             0.0|          (2,[0],[1.0])|\n",
      "| meggy|  F| 30| 35000|         8|  tainan|        0.0|             1.0|          (2,[1],[1.0])|\n",
      "|  adam|  M| 33| 34000|        10|  taipei|        1.0|             2.0|              (2,[],[])|\n",
      "|sophie|  F| 40| 60000|        20|  taipei|        0.0|             2.0|              (2,[],[])|\n",
      "|claire|  F| 25| 30000|         3|  tainan|        0.0|             1.0|          (2,[1],[1.0])|\n",
      "|sherry|  F| 27| 51000|         6|  tainan|        0.0|             1.0|          (2,[1],[1.0])|\n",
      "| phlip|  M| 31| 37000|         7|taichung|        1.0|             0.0|          (2,[0],[1.0])|\n",
      "| fanny|  F| 29| 49000|         5|taichung|        0.0|             0.0|          (2,[0],[1.0])|\n",
      "+------+---+---+------+----------+--------+-----------+----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 2. Handling categorical features by `StringIndexer`, `OneHotEncoder` with `Pipeline`\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "#### only one feature transformation\n",
    "# indexer = StringIndexer(inputCols=['sex','location'], outputCols=['indexed_sex','indexed_location'])\n",
    "# pyspark_df = indexer.fit(pyspark_df).transform(pyspark_df)\n",
    "\n",
    "#### multiple feature transformation with pipeline\n",
    "indexer = [StringIndexer(inputCol=c, outputCol=f'indexed_{c}') for c in ['sex','location']]\n",
    "encoder = [OneHotEncoder(inputCol=c, outputCol=f'onehot_{c}') for c in ['indexed_location']]\n",
    "ppl = Pipeline(stages= indexer + encoder)\n",
    "pyspark_df = ppl.fit(pyspark_df).transform(pyspark_df)\n",
    "pyspark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cdce6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+------+----------+--------+-----------+----------------+-----------------------+--------------------+\n",
      "|  name|sex|age|salary|experience|location|indexed_sex|indexed_location|onehot_indexed_location|            features|\n",
      "+------+---+---+------+----------+--------+-----------+----------------+-----------------------+--------------------+\n",
      "|   amy|  F| 22| 28000|         1|  taipei|        0.0|             2.0|              (2,[],[])|(5,[3,4],[22.0,1.0])|\n",
      "| jacky|  M| 20| 25000|         1|taichung|        1.0|             0.0|          (2,[0],[1.0])|[1.0,1.0,0.0,20.0...|\n",
      "| meggy|  F| 30| 35000|         8|  tainan|        0.0|             1.0|          (2,[1],[1.0])|[0.0,0.0,1.0,30.0...|\n",
      "|  adam|  M| 33| 34000|        10|  taipei|        1.0|             2.0|              (2,[],[])|[1.0,0.0,0.0,33.0...|\n",
      "|sophie|  F| 40| 60000|        20|  taipei|        0.0|             2.0|              (2,[],[])|(5,[3,4],[40.0,20...|\n",
      "|claire|  F| 25| 30000|         3|  tainan|        0.0|             1.0|          (2,[1],[1.0])|[0.0,0.0,1.0,25.0...|\n",
      "|sherry|  F| 27| 51000|         6|  tainan|        0.0|             1.0|          (2,[1],[1.0])|[0.0,0.0,1.0,27.0...|\n",
      "| phlip|  M| 31| 37000|         7|taichung|        1.0|             0.0|          (2,[0],[1.0])|[1.0,1.0,0.0,31.0...|\n",
      "| fanny|  F| 29| 49000|         5|taichung|        0.0|             0.0|          (2,[0],[1.0])|[0.0,1.0,0.0,29.0...|\n",
      "+------+---+---+------+----------+--------+-----------+----------------+-----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 3. Use `VectorAssembler` create new column to assembling the indepent variables for modeling\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['indexed_sex','onehot_indexed_location','age','experience'], outputCol='features')\n",
    "pyspark_df = assembler.transform(pyspark_df)\n",
    "pyspark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b32fbb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. Split into training and testing --> fit model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "finalized_df = pyspark_df.select('features','salary')\n",
    "train_df, test_df = finalized_df.randomSplit([0.7,0.3])\n",
    "regressor = LinearRegression(featuresCol='features', labelCol='salary')\n",
    "regressor = regressor.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bde220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept:  85810.15564896734\n",
      "coefficients:  [-41810.119769775105,24000.000000000586,-6936.706589925221,-3037.9760460453467,9025.317364030418]\n",
      "+--------------------+------+------------------+\n",
      "|            features|salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|(5,[3,4],[40.0,20...| 60000| 144797.4610877618|\n",
      "|[0.0,0.0,1.0,30.0...| 35000| 59936.70658992506|\n",
      "|[0.0,1.0,0.0,29.0...| 49000| 66835.43713380495|\n",
      "|[1.0,1.0,0.0,20.0...| 25000|16265.832322316302|\n",
      "+--------------------+------+------------------+\n",
      "\n",
      "evaluation: -10.546728449185073 34075.94312229387 2051709311.3145728\n"
     ]
    }
   ],
   "source": [
    "#### 5. Get estimated parameters and predictions\n",
    "print('intercept: ', regressor.intercept)\n",
    "print('coefficients: ', regressor.coefficients)\n",
    "\n",
    "pred = regressor.evaluate(test_df)\n",
    "pred.predictions.show()\n",
    "print('evaluation:', pred.r2, pred.meanAbsoluteError, pred.meanSquaredError)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
