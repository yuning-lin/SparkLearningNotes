{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55d6860",
   "metadata": {},
   "source": [
    "## Pyspark DataFrame\n",
    "1. Accessing PySpark from Jupyter Notebook\n",
    "    * jupyter notebook usually accessses the defualt python version\n",
    "    * use `findspark` to find the spark as long as the SPARK_HOME environment varaible is defined\n",
    "    * after `import findspark` and use `findspark.init()` to locate Spark process, we can import pyspark\n",
    "2. Activate the spark cluster with project name: `SparkSession`\n",
    "3. Read csv\n",
    "    * without `inferSchema=True`, the data format will be default string\n",
    "    * `show()` print the dataframe in readable form\n",
    "4. Deal with columns\n",
    "    * get all columns\n",
    "    * select column to transform\n",
    "    * add/drop/rename columns\n",
    "5. Describe data set with summary\n",
    "6. Handling missing value\n",
    "7. Filter operation\n",
    "8. Groupby and aggregate functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abcc8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. Accessing PySpark from Jupyter Notebook\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4703d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AA1900286-NB.corpnet.asus:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_practice_dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dcfcecbd68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 2. Activate the spark cluster with project name: `SparkSession`\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark_practice_dataframe').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818c4785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "None\n",
      "dataframe type:<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+------+---+------+--------+\n",
      "|  name|age|salary|location|\n",
      "+------+---+------+--------+\n",
      "|   amy| 22| 28000|  taipei|\n",
      "| jacky| 20| 25000|taichung|\n",
      "| meggy| 30| 35000|  tainan|\n",
      "|  adam| 33| 34000|  taipei|\n",
      "|sophie| 40| 60000|  taipei|\n",
      "|claire| 25| 30000|  tainan|\n",
      "|sherry| 27| 51000|    null|\n",
      "| phlip| 31|  null|taichung|\n",
      "| fanny| 29|  null|    null|\n",
      "+------+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 3. Read csv\n",
    "pyspark_df = spark.read.csv('./Data/customer.csv', header='true')\n",
    "print(pyspark_df.printSchema())\n",
    "pyspark_df = spark.read.csv('./Data/customer.csv', header='true', inferSchema=True)\n",
    "print(pyspark_df.printSchema())\n",
    "print(f'dataframe type:{type(pyspark_df)}')\n",
    "pyspark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07c80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['name', 'age', 'salary', 'location']\n",
      "\n",
      "head of df:\n",
      "[Row(name='amy', age=22, salary=28000, location='taipei'), Row(name='jacky', age=20, salary=25000, location='taichung'), Row(name='meggy', age=30, salary=35000, location='tainan')]\n",
      "\n",
      "type of seleted columns: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "dtype of seleted columns: [('name', 'string'), ('age', 'int'), ('salary', 'int'), ('location', 'string')]\n",
      "\n",
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|   amy| 22|\n",
      "| jacky| 20|\n",
      "| meggy| 30|\n",
      "|  adam| 33|\n",
      "|sophie| 40|\n",
      "|claire| 25|\n",
      "|sherry| 27|\n",
      "| phlip| 31|\n",
      "| fanny| 29|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 4. Deal with columns (select/head/type)\n",
    "print(f'columns: {pyspark_df.columns}')\n",
    "print()\n",
    "print('head of df:', pyspark_df.head(3), sep='\\n')\n",
    "print()\n",
    "print('type of seleted columns:', type(pyspark_df.select(['name','age'])))\n",
    "print()\n",
    "print('dtype of seleted columns:', pyspark_df.dtypes)\n",
    "print()\n",
    "pyspark_df.select(['name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7bc89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+--------+-------------+\n",
      "|  name|age|salary|location|raised_salary|\n",
      "+------+---+------+--------+-------------+\n",
      "|   amy| 22| 28000|  taipei|      29400.0|\n",
      "| jacky| 20| 25000|taichung|      26250.0|\n",
      "| meggy| 30| 35000|  tainan|      36750.0|\n",
      "|  adam| 33| 34000|  taipei|      35700.0|\n",
      "|sophie| 40| 60000|  taipei|      63000.0|\n",
      "|claire| 25| 30000|  tainan|      31500.0|\n",
      "|sherry| 27| 51000|    null|      53550.0|\n",
      "| phlip| 31|  null|taichung|         null|\n",
      "| fanny| 29|  null|    null|         null|\n",
      "+------+---+------+--------+-------------+\n",
      "\n",
      "+------+---+------+--------+\n",
      "|  name|age|salary|location|\n",
      "+------+---+------+--------+\n",
      "|   amy| 22| 28000|  taipei|\n",
      "| jacky| 20| 25000|taichung|\n",
      "| meggy| 30| 35000|  tainan|\n",
      "|  adam| 33| 34000|  taipei|\n",
      "|sophie| 40| 60000|  taipei|\n",
      "|claire| 25| 30000|  tainan|\n",
      "|sherry| 27| 51000|    null|\n",
      "| phlip| 31|  null|taichung|\n",
      "| fanny| 29|  null|    null|\n",
      "+------+---+------+--------+\n",
      "\n",
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "|   amy| 22|     28000|  taipei|\n",
      "| jacky| 20|     25000|taichung|\n",
      "| meggy| 30|     35000|  tainan|\n",
      "|  adam| 33|     34000|  taipei|\n",
      "|sophie| 40|     60000|  taipei|\n",
      "|claire| 25|     30000|  tainan|\n",
      "|sherry| 27|     51000|    null|\n",
      "| phlip| 31|      null|taichung|\n",
      "| fanny| 29|      null|    null|\n",
      "+------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 4. Deal with columns (add/drop/rename)\n",
    "pyspark_df = pyspark_df.withColumn('raised_salary', pyspark_df['salary']*1.05) # add new column: raised_salary\n",
    "pyspark_df.show()\n",
    "pyspark_df = pyspark_df.drop('raised_salary') # drop column: raised_salary\n",
    "pyspark_df.show()\n",
    "pyspark_df = pyspark_df.withColumnRenamed('salary', 'new_salary') # rename column: salary --> new_salary\n",
    "pyspark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef7eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+------------------+--------+\n",
      "|summary|  name|               age|        new_salary|location|\n",
      "+-------+------+------------------+------------------+--------+\n",
      "|  count|     9|                 9|                 7|       7|\n",
      "|   mean|  null|28.555555555555557| 37571.42857142857|    null|\n",
      "| stddev|  null| 6.023103666530884|12972.498382567419|    null|\n",
      "|    min|  adam|                20|             25000|taichung|\n",
      "|    max|sophie|                40|             60000|  taipei|\n",
      "+-------+------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 5. Describe data set with summary\n",
    "pyspark_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1059987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "|   amy| 22|     28000|  taipei|\n",
      "| jacky| 20|     25000|taichung|\n",
      "| meggy| 30|     35000|  tainan|\n",
      "|  adam| 33|     34000|  taipei|\n",
      "|sophie| 40|     60000|  taipei|\n",
      "|claire| 25|     30000|  tainan|\n",
      "+------+---+----------+--------+\n",
      "\n",
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "|   amy| 22|     28000|  taipei|\n",
      "| jacky| 20|     25000|taichung|\n",
      "| meggy| 30|     35000|  tainan|\n",
      "|  adam| 33|     34000|  taipei|\n",
      "|sophie| 40|     60000|  taipei|\n",
      "|claire| 25|     30000|  tainan|\n",
      "|sherry| 27|     51000|    null|\n",
      "| phlip| 31|      null|taichung|\n",
      "| fanny| 29|      null|    null|\n",
      "+------+---+----------+--------+\n",
      "\n",
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "|   amy| 22|     28000|  taipei|\n",
      "| jacky| 20|     25000|taichung|\n",
      "| meggy| 30|     35000|  tainan|\n",
      "|  adam| 33|     34000|  taipei|\n",
      "|sophie| 40|     60000|  taipei|\n",
      "|claire| 25|     30000|  tainan|\n",
      "|sherry| 27|     51000|    null|\n",
      "| phlip| 31|      null|taichung|\n",
      "+------+---+----------+--------+\n",
      "\n",
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "|   amy| 22|     28000|  taipei|\n",
      "| jacky| 20|     25000|taichung|\n",
      "| meggy| 30|     35000|  tainan|\n",
      "|  adam| 33|     34000|  taipei|\n",
      "|sophie| 40|     60000|  taipei|\n",
      "|claire| 25|     30000|  tainan|\n",
      "|sherry| 27|     51000|    null|\n",
      "+------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 6. Handling missing value (drop)\n",
    "pyspark_df.na.drop().show() # any row with na will be dropped\n",
    "pyspark_df.na.drop(how='all').show() # whole row with na will be dropped\n",
    "pyspark_df.na.drop(how='any', thresh=3).show() # at least 2 not na value will be reserved\n",
    "pyspark_df.na.drop(how='any', subset=['new_salary']).show() # new_salary without na will be reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f61b567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "|   amy| 22|     28000|  taipei|\n",
      "| jacky| 20|     25000|taichung|\n",
      "| meggy| 30|     35000|  tainan|\n",
      "|  adam| 33|     34000|  taipei|\n",
      "|sophie| 40|     60000|  taipei|\n",
      "|claire| 25|     30000|  tainan|\n",
      "|sherry| 27|     51000|    null|\n",
      "| phlip| 31|   -999999|taichung|\n",
      "| fanny| 29|   -999999|    null|\n",
      "+------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 6. Handling missing value (fill with specific number)\n",
    "pyspark_df.na.fill(-999999, ['new_salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d9b5c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+--------+-----------------+----------+\n",
      "|  name|age|new_salary|location|filled_new_salary|filled_age|\n",
      "+------+---+----------+--------+-----------------+----------+\n",
      "|   amy| 22|     28000|  taipei|            28000|        22|\n",
      "| jacky| 20|     25000|taichung|            25000|        20|\n",
      "| meggy| 30|     35000|  tainan|            35000|        30|\n",
      "|  adam| 33|     34000|  taipei|            34000|        33|\n",
      "|sophie| 40|     60000|  taipei|            60000|        40|\n",
      "|claire| 25|     30000|  tainan|            30000|        25|\n",
      "|sherry| 27|     51000|    null|            51000|        27|\n",
      "| phlip| 31|      null|taichung|            37571|        31|\n",
      "| fanny| 29|      null|    null|            37571|        29|\n",
      "+------+---+----------+--------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 6. Handling missing value (fill with mean/median)\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols = ['new_salary','age'],\n",
    "                  outputCols = [f'filled_{c}' for c in ['new_salary','age']]).setStrategy('mean')\n",
    "imputer.fit(pyspark_df).transform(pyspark_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "751d119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "|sophie| 40|     60000|  taipei|\n",
      "|sherry| 27|     51000|    null|\n",
      "+------+---+----------+--------+\n",
      "\n",
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "|sophie| 40|     60000|  taipei|\n",
      "+------+---+----------+--------+\n",
      "\n",
      "+------+---+----------+--------+\n",
      "|  name|age|new_salary|location|\n",
      "+------+---+----------+--------+\n",
      "| jacky| 20|     25000|taichung|\n",
      "| meggy| 30|     35000|  tainan|\n",
      "|claire| 25|     30000|  tainan|\n",
      "| phlip| 31|      null|taichung|\n",
      "+------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 7. Filter operation\n",
    "pyspark_df.filter(pyspark_df['new_salary']>50000).show()\n",
    "pyspark_df.filter((pyspark_df['new_salary']>50000) & (pyspark_df['location']=='taipei')).show()\n",
    "pyspark_df.filter(~(pyspark_df['location']=='taipei')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f1d16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|location|sum(age)|sum(new_salary)|\n",
      "+--------+--------+---------------+\n",
      "|  taipei|      95|         122000|\n",
      "|  tainan|      55|          65000|\n",
      "|    null|      56|          51000|\n",
      "|taichung|      51|          25000|\n",
      "+--------+--------+---------------+\n",
      "\n",
      "+--------+--------+---------------+\n",
      "|location|max(age)|max(new_salary)|\n",
      "+--------+--------+---------------+\n",
      "|  taipei|      40|          60000|\n",
      "|  tainan|      30|          35000|\n",
      "|    null|      29|          51000|\n",
      "|taichung|      31|          25000|\n",
      "+--------+--------+---------------+\n",
      "\n",
      "+--------+---------------+--------+\n",
      "|location|sum(new_salary)|max(age)|\n",
      "+--------+---------------+--------+\n",
      "|  taipei|         122000|      40|\n",
      "|  tainan|          65000|      30|\n",
      "|    null|          51000|      29|\n",
      "|taichung|          25000|      31|\n",
      "+--------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 8. Groupby and aggregate functions\n",
    "pyspark_df.groupBy('location').sum().show()\n",
    "pyspark_df.groupBy('location').max().show()\n",
    "pyspark_df.groupBy('location').agg({'new_salary':'sum','age':'max'}).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
